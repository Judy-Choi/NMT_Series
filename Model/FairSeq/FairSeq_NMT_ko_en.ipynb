{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FairSeq NMT Tutorial\n",
        "**F**acebook **AI R**esearch **Seq**uence-to-Sequence Toolkit written in Python\n",
        "\n",
        "A Fast, Extensible Toolkit for Sequence Modeling"
      ],
      "metadata": {
        "id": "ZMHimrb-O5QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference\n",
        "- https://fairseq.readthedocs.io/en/latest/command_line_tools.html\n",
        "- https://rlqof7ogm.toastcdn.net/references/2021_session_20.pdf\n",
        "- https://github.com/matsunagadaiki151/FairseqTutorial/blob/main/FairseqTranslation.ipynb\n",
        "- https://github.com/Lainshower/jeju-blues/blob/main/Ko-Je%20translation/transformer_translation.ipynb"
      ],
      "metadata": {
        "id": "-QRSd_MXPBC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive & Files"
      ],
      "metadata": {
        "id": "FwptvoyMO1v0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "f-1T2nnq8lAF",
        "outputId": "e3e28397-6260-4163-c787-9b6b4607a234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-17-f3163e8b368f>\", line 7, in <module>\n",
            "    os.symlink('/content/drive/MyDrive/AllforOne/package_collection', my_path)\n",
            "FileExistsError: [Errno 17] File exists: '/content/drive/MyDrive/AllforOne/package_collection' -> '/content/notebooks'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'FileExistsError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "my_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/MyDrive/AllforOne/package_collection', my_path)\n",
        "sys.path.insert(0, my_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba1fnG_n8qEr",
        "outputId": "effa9ad7-b182-4845-92f0-3159ba6bb3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 27 04:26:03 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "AI Hub 'Korean-English pair corpus'  \n",
        "https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=126\n",
        "\n",
        "Same as OpenNMT Tutorial.  \n",
        "https://github.com/Judy-Choi/NMT_Series/blob/main/Model/OpenNMT/NMT_ko-en.ipynb\n",
        "\n",
        "You can download all files from :  \n",
        "https://drive.google.com/drive/folders/1xgNQaaEqJArx3iofoC4JJ8tRmfPSZTTM?usp=share_link\n",
        "\n",
        "\n",
        "### Split Dataset\n",
        "- Val(Dev) : 5,000\n",
        "- Test : 3,000\n",
        "- Train : 1,493,750"
      ],
      "metadata": {
        "id": "VnrR0Fl5PT4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Subword Tokenization\n",
        "Same as OpenNMT Tutorial.\n",
        "- https://github.com/Judy-Choi/NMT_Series/blob/main/Model/OpenNMT/NMT_ko-en.ipynb\n",
        "\n",
        "SentencePiece\n",
        "- Unsupervised text tokenizer and detokenize\n",
        "- Not depend on language\n",
        "- Not depend on Spacing or not\n",
        "- Alleviate the open vocabulary problems (OOV)\n",
        "- Supports **BPE(Byte-Pair-Encoding), Unigram** language model"
      ],
      "metadata": {
        "id": "WaC8C_vVPpc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model\n",
        "### FairSeq\n",
        "https://github.com/facebookresearch/fairseq\n",
        "\n",
        "Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for :\n",
        "- translation\n",
        "- summarization\n",
        "- language modeling\n",
        "- other text generation tasks\n",
        "\n",
        "This toolkit supports\n",
        "- Distributed training\n",
        "across multiple GPUs and machines.\n",
        "- Fast mixed-precision training and inference on modern GPUs\n",
        "- Pytorch"
      ],
      "metadata": {
        "id": "n5nrSZNpQLyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Fairseq"
      ],
      "metadata": {
        "id": "mwH1VOrhNywG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAhdMKXI8sS-",
        "outputId": "cce13d6b-ef82-47a4-9194-bea2045b6513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34534, done.\u001b[K\n",
            "remote: Total 34534 (delta 0), reused 0 (delta 0), pack-reused 34534\u001b[K\n",
            "Receiving objects: 100% (34534/34534), 24.06 MiB | 19.98 MiB/s, done.\n",
            "Resolving deltas: 100% (25109/25109), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH7XGL_28sri",
        "outputId": "eff98f99-995e-4ec2-fdde-aaabd7308e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fairseq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ni6cb_y8tdB",
        "outputId": "9d1d7424-64b8-4187-dbcc-c07f3218b0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CODE_OF_CONDUCT.md  \u001b[0m\u001b[01;34mfairseq\u001b[0m/        LICENSE         RELEASE.md        setup.py\n",
            "CONTRIBUTING.md     \u001b[01;34mfairseq_cli\u001b[0m/    MANIFEST.in     release_utils.py  \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdocs\u001b[0m/               hubconf.py      pyproject.toml  \u001b[01;34mscripts\u001b[0m/          train.py\n",
            "\u001b[01;34mexamples\u001b[0m/           \u001b[01;34mhydra_plugins\u001b[0m/  README.md       setup.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --editable ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZaIBvZH8v3v",
        "outputId": "acb6d0c0-0d73-43a1-9451-2bf580281e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (0.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (1.13.1+cu116)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (2022.10.31)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (0.29.33)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (23.0)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.6/269.6 KB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.9/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->fairseq==0.12.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->fairseq==0.12.2) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->fairseq==0.12.2) (3.1.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp39-cp39-linux_x86_64.whl size=9192 sha256=6d6e3178c1938a14b9ad5f42f49082eec84eaa8d5c02b086e3febe23313cead6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-in9_cbhb/wheels/52/da/57/31b8a8f767e4d044de3fbb1f204d0f1547e8c6e0b171e56bba\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=4705b233aa3ddb180ff25a7cd7333858ffca04e485ae4470d2be14cafbdcd60d\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/3c/ae/14db087e6018de74810afe32eb6ac890ef9c68ba19b00db97a\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "2rQ0K6OpOC_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyproject-toml\n",
        "# For large datasets\n",
        "!pip install pyarrow\n",
        "# For tensorboard log\n",
        "!pip install tensorboardX\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NveO20dh8w_X",
        "outputId": "ca1dbe77-fd5d-465b-d432-e71945baa5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyproject-toml\n",
            "  Downloading pyproject_toml-0.0.10-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from pyproject-toml) (4.3.3)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.9/dist-packages (from pyproject-toml) (67.6.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pyproject-toml) (0.10.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from pyproject-toml) (0.40.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->pyproject-toml) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->pyproject-toml) (0.19.3)\n",
            "Installing collected packages: pyproject-toml\n",
            "Successfully installed pyproject-toml-0.0.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from pyarrow) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (3.19.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses) (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sacremoses) (4.65.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=46d99920236c1af8ef00d0ca70daebe59a75a1d19b31ffcddebac7c4575b4861\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Command-line Tools\n",
        "https://fairseq.readthedocs.io/en/latest/command_line_tools.html\n",
        "\n",
        "Fairseq provides several command-line tools for training and evaluating models:\n",
        "\n",
        "- fairseq-preprocess: Data pre-processing: build vocabularies and binarize training data\n",
        "- fairseq-train: Train a new model on one or multiple GPUs\n",
        "- fairseq-generate: Translate pre-processed data with a trained model\n",
        "- fairseq-interactive: Translate raw text with a trained model\n",
        "- fairseq-score: BLEU scoring of generated translations against reference translations\n",
        "- fairseq-eval-lm: Language model evaluation"
      ],
      "metadata": {
        "id": "jBW49lEFvoRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "uwSCUYr6Oh0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/AllforOne/Lecture/Fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26J_YA9k9Lum",
        "outputId": "3d174b13-2baa-427d-a05b-8334dff2d305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AllforOne/Lecture/Fairseq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/AllforOne/Lecture/Fairseq\""
      ],
      "metadata": {
        "id": "9VPGQdmxOpjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess \\\n",
        "    --source-lang ko \\\n",
        "    --target-lang en \\\n",
        "    --validpref $dir/dataset/valid \\\n",
        "    --trainpref $dir/dataset/train \\\n",
        "    --testpref $dir/dataset/test \\\n",
        "    # --thresholdtgt 3 \\\n",
        "    # --thresholdsrc 3 \\\n",
        "    # --workers 8 \\\n",
        "    --destdir $dir/preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fOcYqVZOqcy",
        "outputId": "a6568375-401a-4a63-f15d-7cb0d6ee0bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-24 15:52:31 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='ko', target_lang='en', trainpref='/content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/train', validpref='/content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/valid', testpref='/content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/test', align_suffix=None, destdir='/content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess', thresholdtgt=3, thresholdsrc=3, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=8, dict_only=False)\n",
            "2023-03-24 15:56:22 | INFO | fairseq_cli.preprocess | [ko] Dictionary: 31176 types\n",
            "2023-03-24 16:00:59 | INFO | fairseq_cli.preprocess | [ko] /content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/train.ko: 1493750 sents, 36585253 tokens, 0.00428% replaced (by <unk>)\n",
            "2023-03-24 16:00:59 | INFO | fairseq_cli.preprocess | [ko] Dictionary: 31176 types\n",
            "2023-03-24 16:01:02 | INFO | fairseq_cli.preprocess | [ko] /content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/valid.ko: 5000 sents, 122498 tokens, 0.00653% replaced (by <unk>)\n",
            "2023-03-24 16:01:02 | INFO | fairseq_cli.preprocess | [ko] Dictionary: 31176 types\n",
            "2023-03-24 16:01:05 | INFO | fairseq_cli.preprocess | [ko] /content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/test.ko: 3000 sents, 74132 tokens, 0.0054% replaced (by <unk>)\n",
            "2023-03-24 16:01:05 | INFO | fairseq_cli.preprocess | [en] Dictionary: 31816 types\n",
            "2023-03-24 16:05:50 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/train.en: 1493750 sents, 44524156 tokens, 0.00069% replaced (by <unk>)\n",
            "2023-03-24 16:05:50 | INFO | fairseq_cli.preprocess | [en] Dictionary: 31816 types\n",
            "2023-03-24 16:05:54 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/valid.en: 5000 sents, 148544 tokens, 0.0% replaced (by <unk>)\n",
            "2023-03-24 16:05:54 | INFO | fairseq_cli.preprocess | [en] Dictionary: 31816 types\n",
            "2023-03-24 16:05:56 | INFO | fairseq_cli.preprocess | [en] /content/drive/MyDrive/AllforOne/Lecture/Fairseq/dataset/test.en: 3000 sents, 90635 tokens, 0.0% replaced (by <unk>)\n",
            "2023-03-24 16:05:56 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "9XGqOrhWREBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### fairseq-train arguments\n",
        "Example\n",
        "\n",
        "```\n",
        "fairseq-train \\\n",
        "    [preprocess_dir] \\\n",
        "    --source-lang ko \\\n",
        "    --target-lang en \\\n",
        "    --arch transformer \\\n",
        "    --max-sentences 50 \\\n",
        "    --optimizer adam\n",
        "    --save-dir [checkpoint_dir \\\n",
        "```\n",
        "\n",
        "**Model**  \n",
        "- --arch [model]   \n",
        "- --optimizer [optimizer]   \n",
        "- --max-epoch [force stop training at specified epoch]   \n",
        "- --batch-size, --max-sentences [number of examples in a batch]   \n",
        "\n",
        "**Set validation metric (ex: BLEU)**   \n",
        "- --scoring [scoreing metric]   \n",
        "- --best-checkpoint-metric [metric to use for saving “best” checkpoints]   \n",
        "\n",
        "(You should add these arguments too.  \n",
        "Only '--best-checkpoint-metric' bleu flag doesn't work alone)\n",
        "- --eval-bleu\n",
        "- --eval-bleu-args\n",
        "- --eval-bleu-detok moses\n",
        "- --eval-bleu-remove-bpe\n",
        "- --eval-bleu-print-samples\n",
        "\n",
        "**Save checkpoint**\n",
        "- --maximize-best-checkpoint-metric\n",
        "  - select the largest metric value for saving “best” checkpoints\n",
        "- --no-epoch-checkpoints\n",
        "  - only store last and best checkpoints\n",
        "- --save-dir [path to save checkpoints]"
      ],
      "metadata": {
        "id": "dsYVKDvVv6xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/AllforOne/Lecture/Fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBw8WwzBRTkd",
        "outputId": "f0c7c411-5cd4-4de1-9222-6edbd7b15507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AllforOne/Lecture/Fairseq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-train /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess \\\n",
        "    --source-lang ko \\\n",
        "    --target-lang en \\\n",
        "    --task translation \\\n",
        "    --arch transformer \\\n",
        "    --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr 0.0005 \\\n",
        "    --lr-scheduler inverse_sqrt \\\n",
        "    --label-smoothing 0.1 \\\n",
        "    --dropout 0.3 \\\n",
        "    --max-tokens 4000 \\\n",
        "    --stop-min-lr '1e-09' \\\n",
        "    --lr-scheduler inverse_sqrt       \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy       \\\n",
        "    --warmup-updates 4000 \\\n",
        "    --warmup-init-lr '1e-07'    \\\n",
        "    --skip-invalid-size-inputs-valid-test \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --max-epoch 100 \\\n",
        "    --no-epoch-checkpoints \\\n",
        "    --save-dir \"/content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XkfW9HWnKo2",
        "outputId": "85ba0be4-a5e1-4a6b-a855-66df8f395fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-27 09:16:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=4000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=100, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=1e-09, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='/content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess', source_lang='ko', target_lang='en', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=True, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': '/content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess', 'source_lang': 'ko', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-03-27 09:16:28 | INFO | fairseq.tasks.translation | [ko] dictionary: 31176 types\n",
            "2023-03-27 09:16:28 | INFO | fairseq.tasks.translation | [en] dictionary: 31816 types\n",
            "2023-03-27 09:16:30 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(31176, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(31816, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=31816, bias=False)\n",
            "  )\n",
            ")\n",
            "2023-03-27 09:16:30 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-03-27 09:16:30 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2023-03-27 09:16:30 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-03-27 09:16:30 | INFO | fairseq_cli.train | num. shared model params: 76,390,400 (num. trained: 76,390,400)\n",
            "2023-03-27 09:16:30 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-03-27 09:16:30 | INFO | fairseq.data.data_utils | loaded 5,000 examples from: /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/valid.ko-en.ko\n",
            "2023-03-27 09:16:30 | INFO | fairseq.data.data_utils | loaded 5,000 examples from: /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/valid.ko-en.en\n",
            "2023-03-27 09:16:30 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess valid ko-en 5000 examples\n",
            "2023-03-27 09:16:31 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2023-03-27 09:16:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-03-27 09:16:31 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-03-27 09:16:31 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-03-27 09:16:31 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-03-27 09:16:31 | INFO | fairseq_cli.train | max tokens per device = 4000 and max sentences per device = None\n",
            "2023-03-27 09:16:31 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_last.pt\n",
            "2023-03-27 09:16:34 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-03-27 09:16:35 | INFO | fairseq.trainer | Loaded checkpoint /content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_last.pt (epoch 5 @ 47296 updates)\n",
            "2023-03-27 09:16:35 | INFO | fairseq.trainer | loading train data for epoch 5\n",
            "2023-03-27 09:16:35 | INFO | fairseq.data.data_utils | loaded 1,493,750 examples from: /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/train.ko-en.ko\n",
            "2023-03-27 09:16:35 | INFO | fairseq.data.data_utils | loaded 1,493,750 examples from: /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/train.ko-en.en\n",
            "2023-03-27 09:16:35 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess train ko-en 1493750 examples\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 5\n",
            "2023-03-27 09:16:36 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-03-27 09:16:36 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-03-27 09:16:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 11824\n",
            "epoch 005:   0% 0/11824 [00:00<?, ?it/s]2023-03-27 09:16:37 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-03-27 09:16:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 005: 100% 11823/11824 [1:27:15<00:00,  2.27it/s, loss=4.187, nll_loss=2.598, ppl=6.05, wps=8514, ups=2.26, wpb=3769.6, bsz=112.1, num_updates=59100, lr=0.000130079, gnorm=1.236, train_wall=44, gb_free=11.1, wall=5233]2023-03-27 10:43:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "2023-03-27 10:43:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/60 [00:00<?, ?it/s]\u001b[A2023-03-27 10:43:54 | INFO | fairseq.tasks.translation | example hypothesis: ▁I ▁uploaded ▁the ▁data ▁needed ▁for ▁the ▁analysis.\n",
            "2023-03-27 10:43:54 | INFO | fairseq.tasks.translation | example reference: ▁I ▁uploaded ▁the ▁data ▁that ▁needs ▁for ▁analysis.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   2% 1/60 [00:01<00:59,  1.01s/it]\u001b[A2023-03-27 10:43:55 | INFO | fairseq.tasks.translation | example hypothesis: ▁I ▁attach ▁the ▁file ▁to ▁the ▁email.\n",
            "2023-03-27 10:43:55 | INFO | fairseq.tasks.translation | example reference: ▁I 'll ▁attach ▁the ▁file ▁to ▁the ▁mail.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   3% 2/60 [00:01<00:55,  1.04it/s]\u001b[A2023-03-27 10:43:56 | INFO | fairseq.tasks.translation | example hypothesis: ▁I ▁can ▁meet ▁you ▁on ▁Wednesday ▁afternoon ▁on ▁weekdays.\n",
            "2023-03-27 10:43:56 | INFO | fairseq.tasks.translation | example reference: ▁I ▁can ▁meet ▁you ▁on ▁Wednesday ▁afternoon ▁during ▁the ▁weekdays.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   5% 3/60 [00:03<01:03,  1.12s/it]\u001b[A2023-03-27 10:43:57 | INFO | fairseq.tasks.translation | example hypothesis: ▁Look ▁at ▁how ▁much ▁free ▁perfume ▁will ▁arrive ▁at ▁home.\n",
            "2023-03-27 10:43:57 | INFO | fairseq.tasks.translation | example reference: ▁See ▁how ▁much ▁free ▁perfume ▁shows ▁up ▁at ▁your ▁home.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   7% 4/60 [00:04<00:59,  1.06s/it]\u001b[A2023-03-27 10:43:58 | INFO | fairseq.tasks.translation | example hypothesis: ▁I ▁will ▁give ▁you ▁a ▁reply ▁tomorrow ▁after ▁discussing ▁with ▁Mr. ▁Kim.\n",
            "2023-03-27 10:43:58 | INFO | fairseq.tasks.translation | example reference: ▁I ▁will ▁answer ▁you ▁tomorrow ▁after ▁a ▁discussion ▁with ▁Kim.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   8% 5/60 [00:05<00:57,  1.05s/it]\u001b[A2023-03-27 10:43:59 | INFO | fairseq.tasks.translation | example hypothesis: ▁That 's ▁just ▁another ▁thing.\n",
            "2023-03-27 10:43:59 | INFO | fairseq.tasks.translation | example reference: ▁It 's ▁just ▁another ▁different ▁one.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  10% 6/60 [00:06<00:54,  1.00s/it]\u001b[A2023-03-27 10:44:01 | INFO | fairseq.tasks.translation | example hypothesis: ▁I ▁came ▁down ▁to ▁Changwon ▁yesterday ▁and ▁I 'm ▁playing ▁golf.\n",
            "2023-03-27 10:44:01 | INFO | fairseq.tasks.translation | example reference: ▁I ▁came ▁to ▁Changwon ▁yesterday ▁to ▁play ▁golf ▁and ▁currently ▁returning ▁back ▁to ▁Seoul.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  12% 7/60 [00:07<00:57,  1.08s/it]\u001b[A2023-03-27 10:44:02 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" A ▁total ▁of ▁15 9,000 ▁text ▁messages ▁she ▁sent ▁were ▁found ▁to ▁have ▁sent .\"\n",
            "2023-03-27 10:44:02 | INFO | fairseq.tasks.translation | example reference: ▁\" A ▁total ▁of ▁15 9,000 ▁text ▁messages ▁she ▁sent ▁were ▁surveyed .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  13% 8/60 [00:08<00:55,  1.06s/it]\u001b[A2023-03-27 10:44:03 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Of ▁course, ▁consumers ▁can 't ▁buy ▁it ▁because ▁they ▁believe ▁that ▁storage ▁cases ▁are ▁included .\"\n",
            "2023-03-27 10:44:03 | INFO | fairseq.tasks.translation | example reference: ▁Consumers ▁make ▁a ▁purchase ▁naturally ▁believing ▁that ▁the ▁storage ▁case ▁is ▁included.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  15% 9/60 [00:09<00:52,  1.03s/it]\u001b[A2023-03-27 10:44:04 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Due ▁to ▁the ▁danger ▁of ▁damage, ▁a ▁two - sided ▁tape ▁is ▁attached ▁on ▁the ▁back ▁of ▁the ▁watch .\"\n",
            "2023-03-27 10:44:04 | INFO | fairseq.tasks.translation | example reference: ▁Double - sided ▁tape ▁is ▁attached ▁to ▁the ▁backside ▁of ▁the ▁clock ▁as ▁there ▁is ▁a ▁risk ▁of ▁damage.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  17% 10/60 [00:10<00:53,  1.07s/it]\u001b[A2023-03-27 10:44:05 | INFO | fairseq.tasks.translation | example hypothesis: ▁I 'm ▁going ▁to ▁talk ▁about ▁the ▁medical ▁accident ▁I' m ▁interested ▁in ▁today.\n",
            "2023-03-27 10:44:05 | INFO | fairseq.tasks.translation | example reference: ▁\" Today, ▁I ▁would ▁like ▁to ▁talk ▁about ▁the ▁medical ▁accident ▁which ▁I ▁am ▁interested ▁in .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  18% 11/60 [00:11<00:52,  1.08s/it]\u001b[A2023-03-27 10:44:06 | INFO | fairseq.tasks.translation | example hypothesis: ▁This ▁is ▁because ▁it ▁is ▁important ▁to ▁boost ▁corporate ▁spirit ▁for ▁economic ▁growth.\n",
            "2023-03-27 10:44:06 | INFO | fairseq.tasks.translation | example reference: ▁ Because ▁we ▁think ▁it 's ▁important ▁to ▁raise ▁ entrepreneurship ▁for ▁economic ▁growth.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  20% 12/60 [00:12<00:51,  1.07s/it]\u001b[A2023-03-27 10:44:07 | INFO | fairseq.tasks.translation | example hypothesis: ▁I ▁think ▁it ▁was ▁the ▁hardest ▁task ▁to ▁make ▁a ▁15 - minute ▁movie ▁together.\n",
            "2023-03-27 10:44:07 | INFO | fairseq.tasks.translation | example reference: ▁Making ▁a ▁15 - minute ▁film ▁was ▁the ▁hardest ▁group ▁project ▁for ▁me.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  22% 13/60 [00:13<00:51,  1.10s/it]\u001b[A2023-03-27 10:44:08 | INFO | fairseq.tasks.translation | example hypothesis: ▁The ▁Korea ▁Ice ▁Games ▁Federation ▁reported ▁false ▁reports ▁that ▁Shim ▁Suk - hee ▁was ▁caught ▁in ▁a ▁cold.\n",
            "2023-03-27 10:44:08 | INFO | fairseq.tasks.translation | example reference: ▁The ▁Korea ▁Skating ▁Union ▁reported ▁falsely ▁that ▁Shim ▁Suk - hee ▁had ▁a ▁cold.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  23% 14/60 [00:15<00:53,  1.16s/it]\u001b[A2023-03-27 10:44:10 | INFO | fairseq.tasks.translation | example hypothesis: ▁Can ▁Ma il s ▁forget ▁his ▁ex - wife ▁and ▁become ▁a ▁new ▁lover ▁with ▁Ma ya?\n",
            "2023-03-27 10:44:10 | INFO | fairseq.tasks.translation | example reference: ▁Would ▁Mi les ▁be ▁able ▁to ▁forget ▁his ▁ex - wife ▁and ▁get ▁with ▁May a?\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  25% 15/60 [00:16<00:54,  1.20s/it]\u001b[A2023-03-27 10:44:11 | INFO | fairseq.tasks.translation | example hypothesis: ▁Try ▁the ▁strength ▁and ▁weaknesses ▁of ▁a ▁single ▁box ▁design ▁from ▁the ▁perspective ▁of ▁internal ▁feasibility ▁and ▁external ▁feasibility.\n",
            "2023-03-27 10:44:11 | INFO | fairseq.tasks.translation | example reference: ▁Identify ▁the ▁strengths ▁and ▁weaknesses ▁of ▁single - case ▁designs ▁from ▁the ▁standpoint ▁of ▁internal ▁and ▁external ▁validity.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  27% 16/60 [00:17<00:52,  1.19s/it]\u001b[A2023-03-27 10:44:12 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" As ▁many ▁as ▁1.7 ▁million ▁people ▁will ▁be ▁eligible, ▁the ▁immigration ▁policy ▁research ▁institute ▁and ▁the ▁P ix e ▁Center ▁said .\"\n",
            "2023-03-27 10:44:12 | INFO | fairseq.tasks.translation | example reference: ▁\" As ▁many ▁as ▁1.7 ▁million ▁could ▁be ▁eligible, ▁the ▁Migration ▁Policy ▁Institute ▁and ▁Pe w ▁His pan ic ▁Center ▁said .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  28% 17/60 [00:18<00:50,  1.18s/it]\u001b[A2023-03-27 10:44:13 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Jang ▁Hee - young, ▁a ▁senior ▁member ▁and ▁singer ▁of ▁the ▁group, ▁becomes ▁the ▁bride ▁of ▁August .\"\n",
            "2023-03-27 10:44:13 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁original ▁member ▁of ▁Group ▁Ga vy ▁N J ▁and ▁a ▁singer, ▁Jang ▁Hee - young, ▁will ▁become ▁the ▁bride ▁of ▁August .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  30% 18/60 [00:19<00:48,  1.16s/it]\u001b[A2023-03-27 10:44:14 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁area, ▁including ▁Song san ▁Mountain ▁in ▁Hwaseong, ▁has ▁a ▁land ▁of ▁30 ▁million ▁pyeong ▁and ▁a ▁dinosaur ▁egg ▁fossil ▁of ▁45 ▁million ▁pyeong .\"\n",
            "2023-03-27 10:44:14 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁area ▁around ▁Songsan, ▁Hwaseong, ▁has ▁an ▁area ▁of ▁30 ▁million ▁pyeong, ▁and ▁fossil ▁dinosaur ▁eggs ▁are ▁4.5 ▁million ▁pyeong .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  32% 19/60 [00:21<00:48,  1.17s/it]\u001b[A2023-03-27 10:44:15 | INFO | fairseq.tasks.translation | example hypothesis: ▁The ▁news ▁is ▁that ▁representatives ▁from ▁more ▁than ▁200 ▁countries ▁around ▁the ▁world ▁have ▁succeeded ▁in ▁drawing ▁up ▁detailed ▁guidelines ▁to ▁prevent ▁global ▁warming.\n",
            "2023-03-27 10:44:15 | INFO | fairseq.tasks.translation | example reference: ▁It ▁is ▁reported ▁that ▁representatives ▁from ▁more ▁than ▁200 ▁countries ▁have ▁succeeded ▁barely ▁to ▁come ▁up ▁with ▁detailed ▁implementation ▁guidelines ▁to ▁prevent ▁global ▁warming.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  33% 20/60 [00:22<00:46,  1.16s/it]\u001b[A2023-03-27 10:44:16 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁user ▁fees ▁referred ▁to ▁in ▁paragraph ▁(1) ▁may ▁be ▁user ▁fees ▁within ▁the ▁scope ▁prescribed ▁in ▁attached ▁Table ▁2, ▁and ▁detailed ▁matters ▁concerning ▁such ▁use ▁shall ▁be ▁prescribed ▁by ▁the ▁Rules .\"\n",
            "2023-03-27 10:44:16 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁usage ▁fees ▁under ▁paragraph ▁(1) ▁may ▁be ▁collected ▁within ▁the ▁extent ▁prescribed ▁in ▁the ▁attached ▁Table ▁2, ▁and ▁detailed ▁matters ▁concerning ▁such ▁usage ▁fees ▁shall ▁be ▁prescribed ▁by ▁the ▁Rules .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  35% 21/60 [00:23<00:44,  1.13s/it]\u001b[A2023-03-27 10:44:18 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" At ▁one ▁point, ▁I S, ▁which ▁took ▁control ▁of ▁the ▁territory ▁as ▁much ▁as ▁the ▁size ▁of ▁the ▁UK, ▁is ▁now ▁protesting ▁the ▁small ▁rural ▁villages ▁in ▁Syria .\"\n",
            "2023-03-27 10:44:18 | INFO | fairseq.tasks.translation | example reference: ▁\" IS, ▁which ▁once ▁dominated ▁the ▁land ▁as ▁large ▁as ▁the ▁size ▁of ▁Britain, ▁is ▁now ▁surrounded ▁and ▁is ▁resisting ▁in ▁a ▁small ▁rural ▁village ▁in ▁Syria .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  37% 22/60 [00:24<00:42,  1.11s/it]\u001b[A2023-03-27 10:44:19 | INFO | fairseq.tasks.translation | example hypothesis: ▁The ▁visit ▁to ▁Gwangju ▁on ▁the ▁2 nd ▁is ▁due ▁to ▁the ▁Gwangju ▁Metropolitan ▁City 's ▁selection ▁of ▁the ▁Korea' s ▁best ▁public ▁administration ▁agency ▁under ▁the ▁supervision ▁of ▁the ▁Ministry ▁of ▁Public ▁Administration ▁and ▁Security.\n",
            "2023-03-27 10:44:19 | INFO | fairseq.tasks.translation | example reference: ▁The ▁two - day ▁visit ▁to ▁Gwangju ▁by ▁the ▁diplomatic ▁corps ▁in ▁Korea ▁comes ▁after ▁Gwangju ▁City ▁was ▁selected ▁as ▁one ▁of ▁South ▁Korea 's ▁best ▁public ▁administration ▁institutions ▁organized ▁by ▁the ▁Ministry ▁of ▁Public ▁Administration ▁and ▁Security.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  38% 23/60 [00:25<00:40,  1.09s/it]\u001b[A2023-03-27 10:44:20 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" In ▁the ▁case ▁of ▁the ▁opposition ▁party, ▁the ▁leader ▁of ▁the ▁People 's ▁Party ▁and ▁the ▁Democratic ▁Party ▁of ▁Korea ▁concluded ▁that ▁there ▁was ▁no ▁strategic ▁solidarity .\"\n",
            "2023-03-27 10:44:20 | INFO | fairseq.tasks.translation | example reference: ▁The ▁opposition ▁party ▁experienced ▁a ▁unification ▁dispute ▁because ▁the ▁Kookmin eui ▁Party ▁and ▁De o bul eo min ju ▁Party ▁leaders ▁said ▁that ▁there ▁would ▁be ▁no ▁strategic ▁alliance.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  40% 24/60 [00:26<00:37,  1.06s/it]\u001b[A2023-03-27 10:44:21 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" More ▁than ▁260 ▁artists, ▁including ▁Korean ▁artists ▁as ▁well ▁as ▁Western ▁artists ▁such ▁as ▁the ▁U. S. ▁and ▁Finland, ▁will ▁participate ▁to ▁explore ▁the ▁international ▁possibilities ▁of ▁art ▁as ▁art ▁genres .\"\n",
            "2023-03-27 10:44:21 | INFO | fairseq.tasks.translation | example reference: ▁\" In ▁addition ▁to ▁Korean ▁artists, ▁more ▁than ▁260 ▁artists ▁from ▁Western ▁countries ▁such ▁as ▁England, ▁Finland, ▁and ▁the ▁United ▁States ▁participate ▁and ▁study ▁the ▁international ▁possibilities ▁of ▁ink ▁as ▁an ▁art ▁genre .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  42% 25/60 [00:27<00:39,  1.12s/it]\u001b[A2023-03-27 10:44:22 | INFO | fairseq.tasks.translation | example hypothesis: ▁The ▁Mayor ▁may ▁entrust ▁juvenile ▁organizations ▁or ▁non - profit ▁corporations ▁with ▁the ▁management ▁and ▁operation ▁of ▁facilities ▁to ▁secure ▁expertise ▁in ▁the ▁efficient ▁management ▁and ▁operation ▁of ▁facilities.\n",
            "2023-03-27 10:44:22 | INFO | fairseq.tasks.translation | example reference: ▁The ▁Mayor ▁may ▁entrust ▁juvenile ▁organizations ▁or ▁non profit ▁corporations ▁with ▁the ▁management ▁and ▁operation ▁of ▁their ▁facilities ▁in ▁order ▁to ▁ensure ▁the ▁efficient ▁management ▁and ▁operation ▁of ▁facilities.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  43% 26/60 [00:29<00:41,  1.21s/it]\u001b[A2023-03-27 10:44:24 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁head ▁of ▁the ▁Gu ▁shall ▁assign ▁cultural ▁tour ▁commentators ▁on ▁a ▁net ▁basis ▁in ▁consideration ▁of ▁the ▁scale ▁of ▁tourists, ▁tourists, ▁etc. ▁and ▁the ▁performance, ▁attitude, ▁sincerity, ▁etc. ▁of ▁cultural ▁tour ▁commentators .\"\n",
            "2023-03-27 10:44:24 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁head ▁of ▁the ▁Gu ▁shall ▁place ▁cultural ▁tour ▁commentators ▁by ▁a ▁rotating ▁system, ▁taking ▁into ▁account ▁the ▁scale ▁of ▁tourists, ▁the ▁performance ▁and ▁attitudes ▁of ▁cultural ▁tour ▁commentators, ▁their ▁level ▁of ▁faithfulness, ▁etc .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  45% 27/60 [00:30<00:42,  1.29s/it]\u001b[A2023-03-27 10:44:25 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Most ▁of ▁the ▁ships ▁are ▁deployed ▁according ▁to ▁long - term ▁transportation ▁contracts, ▁but ▁if ▁LPG ▁transportation ▁market ▁rises ▁in ▁the ▁long ▁term, ▁short - term ▁earnings ▁rebound ▁is ▁expected .\"\n",
            "2023-03-27 10:44:25 | INFO | fairseq.tasks.translation | example reference: ▁\" Most ▁ships ▁are ▁arranged ▁under ▁long - term ▁contracts, ▁but ▁if ▁the ▁LPG ▁transport ▁market ▁rises ▁over ▁the ▁long ▁term, ▁short - term ▁earnings ▁rebound ▁is ▁also ▁expected .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  47% 28/60 [00:31<00:40,  1.27s/it]\u001b[A2023-03-27 10:44:26 | INFO | fairseq.tasks.translation | example hypothesis: ▁The ▁trust ▁funds ▁entrusted ▁to ▁the ▁bank ▁by ▁the ▁customer ▁will ▁be ▁delivered ▁after ▁confirming ▁the ▁identity ▁of ▁the ▁North ▁Korean ▁family ▁at ▁a ▁time ▁when ▁funds ▁can ▁be ▁moved ▁by ▁forming ▁smooth ▁exchanges ▁between ▁the ▁two ▁Korea s ▁after ▁unification.\n",
            "2023-03-27 10:44:26 | INFO | fairseq.tasks.translation | example reference: ▁Trust ▁funds ▁entrusted ▁by ▁customers ▁to ▁banks ▁will ▁be ▁delivered ▁after ▁unification ▁or ▁at ▁a ▁time ▁when ▁the ▁funds ▁can ▁be ▁transferred ▁through ▁smooth ▁exchanges ▁between ▁the ▁two ▁Korea s ▁after ▁identification ▁of ▁the ▁North ▁Korean ▁family.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  48% 29/60 [00:33<00:40,  1.31s/it]\u001b[A2023-03-27 10:44:27 | INFO | fairseq.tasks.translation | example hypothesis: ▁The ▁Supreme ▁Court 's ▁judgment ▁on ▁how ▁to ▁define ▁the ▁age ▁of ▁physical ▁labor ▁and ▁the ▁so - called ▁working ▁age ▁has ▁changed ▁in ▁30 ▁years.\n",
            "2023-03-27 10:44:27 | INFO | fairseq.tasks.translation | example reference: ▁\" How ▁to ▁define ▁the ▁age ▁at ▁which ▁manual ▁work ▁is ▁possible, ▁the ▁Supreme ▁Court 's ▁judgment ▁in ▁the ▁so - called ▁working ▁age ▁has ▁changed ▁in ▁30 ▁years .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  50% 30/60 [00:34<00:36,  1.23s/it]\u001b[A2023-03-27 10:44:29 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" As ▁growth ▁is ▁expected ▁due ▁to ▁the ▁expansion ▁of ▁domestic ▁and ▁foreign ▁production ▁bases, ▁the ▁bio ▁sector ▁is ▁also ▁said ▁to ▁have ▁increased ▁performance ▁stability ▁due ▁to ▁improved ▁cost ▁and ▁increased ▁sales ▁of ▁high - profit ▁items .\"\n",
            "2023-03-27 10:44:29 | INFO | fairseq.tasks.translation | example reference: ▁\" We ▁expect ▁growth ▁due ▁to ▁the ▁expansion ▁of ▁production ▁bases ▁at ▁home ▁and ▁abroad, ▁and ▁the ▁bio ▁sector 's ▁earnings ▁stability ▁has ▁also ▁improved ▁due ▁to ▁cost ▁improvement ▁and ▁increased ▁sales ▁of ▁high - margin ▁items .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  52% 31/60 [00:35<00:37,  1.28s/it]\u001b[A2023-03-27 10:44:30 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" His ▁claim ▁is ▁that ▁Da ko ▁is ▁likely ▁to ▁be ▁a ▁passive ▁alternative ▁to ▁projects ▁that ▁have ▁failed ▁to ▁make ▁success, ▁rather ▁than ▁the ▁result ▁of ▁active ▁choices .\"\n",
            "2023-03-27 10:44:30 | INFO | fairseq.tasks.translation | example reference: ▁\" He ▁claims ▁that ▁there ▁is ▁a ▁high ▁possibility ▁for ▁DA ICO ▁to ▁become ▁the ▁passive ▁solution ▁for ▁projects ▁with ▁fear ▁of ▁failure, ▁rather ▁than ▁a ▁result ▁of ▁active ▁choice .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  53% 32/60 [00:36<00:33,  1.21s/it]\u001b[A2023-03-27 10:44:31 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" In ▁particular, ▁on ▁the ▁broadcast ▁on ▁this ▁day, ▁the ▁first ▁appearance ▁of ▁soccer ▁player ▁Park ▁Joo - ho, ▁who ▁joined ▁as ▁a ▁new ▁family ▁member ▁of ▁\"\" S al dol ,\"\" ▁and ▁good - hoo ▁brothers ▁and ▁sisters ▁will ▁be ▁depicted .\"\n",
            "2023-03-27 10:44:31 | INFO | fairseq.tasks.translation | example reference: ▁\" In ▁particular, ▁the ▁broadcast ▁shows ▁the ▁first ▁appearances ▁of ▁soccer ▁player ▁Park ▁Ju - ho ▁and ▁the ▁sibling ▁Nae - eu ▁and ▁Gun - hu, ▁who ▁are ▁the ▁new ▁members ▁of ▁\"\" Superman ▁is ▁back .\"\"\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  55% 33/60 [00:38<00:34,  1.28s/it]\u001b[A2023-03-27 10:44:32 | INFO | fairseq.tasks.translation | example hypothesis: ▁He ▁led ▁the ▁play ▁with ▁delicate ▁emotional ▁performance ▁from ▁his ▁full ▁self - esteem ▁of ▁whether ▁hidden ▁secrets ▁will ▁be ▁revealed ▁to ▁his ▁daughter ▁as ▁well ▁as ▁his ▁father 's ▁appearance ▁that ▁does ▁not ▁care ▁for ▁his ▁daughter.\n",
            "2023-03-27 10:44:32 | INFO | fairseq.tasks.translation | example reference: ▁The ▁delicate ▁emotional ▁performance ▁of ▁the ▁father ▁who ▁is ▁generous ▁with ▁advice ▁for ▁his ▁daughter ▁and ▁is ▁on ▁edge ▁worrying ▁about ▁the ▁disclosure ▁of ▁the ▁hidden ▁truth ▁all ▁led ▁the ▁drama.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  57% 34/60 [00:39<00:32,  1.24s/it]\u001b[A2023-03-27 10:44:34 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Jang ▁Young - geun, ▁head ▁of ▁the ▁corporate ▁policy ▁division ▁in ▁the ▁province, ▁said, ▁“ We ▁plan ▁to ▁expand ▁mutual ▁exchanges ▁so ▁that ▁the ▁successful ▁senior ▁CEO 's ▁experience ▁in ▁starting ▁a ▁business ▁can ▁be ▁self - esteem ▁for ▁juniors. ”\"\n",
            "2023-03-27 10:44:34 | INFO | fairseq.tasks.translation | example reference: ▁\" Jang ▁Young - Geun, ▁head ▁of ▁Corporate ▁Policy ▁Division, ▁said, ▁“ We ▁will ▁continue ▁to ▁expand ▁our ▁interactions ▁so ▁that ▁senior ▁CEO 's ▁experience ▁in ▁starting ▁a ▁business ▁is ▁ nourished ▁by ▁younger ▁employees .\"\"\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  58% 35/60 [00:40<00:32,  1.30s/it]\u001b[A2023-03-27 10:44:35 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" If ▁the ▁relationship ▁between ▁the ▁two ▁former ▁chiefs ▁and ▁the ▁former ▁deputy ▁chief ▁Lim ▁is ▁not ▁clearly ▁proven, ▁it ▁could ▁be ▁more ▁difficult ▁to ▁hold ▁the ▁sins ▁of ▁the ▁former ▁Chief ▁Justice ▁of ▁the ▁Supreme ▁Court ▁Yang ▁in ▁the ▁case .\"\n",
            "2023-03-27 10:44:35 | INFO | fairseq.tasks.translation | example reference: ▁\" If ▁the ▁collusion ▁between ▁the ▁two ▁other ▁former ▁chiefs ▁and ▁Lim ▁is ▁not ▁clearly ▁established, ▁it ▁could ▁become ▁more ▁difficult ▁to ▁accuse ▁former ▁Supreme ▁Court ▁Chief ▁Justice ▁Yang ▁in ▁this ▁case .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  60% 36/60 [00:41<00:29,  1.25s/it]\u001b[A2023-03-27 10:44:37 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁delivery ▁competition ▁among ▁retailers ▁has ▁spread ▁to ▁the ▁early ▁morning ▁delivery ▁of ▁fresh ▁food, ▁followed ▁by ▁regular ▁regular ▁delivery ▁in ▁the ▁form ▁of ▁side ▁dishes ▁centered ▁on ▁side ▁dishes .\"\n",
            "2023-03-27 10:44:37 | INFO | fairseq.tasks.translation | example reference: ▁Not ▁only ▁that ▁the ▁distribution ▁companies' ▁competition ▁for ▁delivery ▁has ▁spread ▁to ▁the ▁early ▁morning ▁delivery ▁of ▁fresh ▁food ▁but ▁also ▁they ▁are ▁expanding ▁its ▁scope ▁to ▁regular ▁delivery ▁in ▁the ▁form ▁of ▁subscription ▁centering ▁on ▁the ▁category ▁of ▁side ▁dishes.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  62% 37/60 [00:43<00:31,  1.39s/it]\u001b[A2023-03-27 10:44:38 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Up on ▁arriving ▁at ▁the ▁scene, ▁Ho s bot ▁moves ▁again, ▁leaving ▁the ▁car ▁discuss bot ▁in ▁the ▁place ▁and ▁increasing ▁the ▁range ▁of ▁fire ▁engines ▁or ▁fire ▁fighting ▁places .\"\n",
            "2023-03-27 10:44:38 | INFO | fairseq.tasks.translation | example reference: ▁\" Up on ▁arrival ▁at ▁the ▁scene, ▁the ▁Ho se ▁bot ▁will ▁leave ▁the ▁Can non ▁bot ▁in ▁place ▁and ▁move ▁the ▁hose ▁back ▁down ▁to ▁where ▁the ▁fire ▁truck ▁or ▁fire ▁hydrant ▁is .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  63% 38/60 [00:44<00:31,  1.41s/it]\u001b[A2023-03-27 10:44:40 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" If ▁a ▁person ▁who ▁is ▁not ▁a ▁victim ▁party ▁attract s ▁investment ▁in ▁the ▁name ▁of ▁the ▁cost ▁of ▁excavation ▁with ▁a ▁lie ▁that ▁he / she ▁found ▁a ▁treasure ▁ship, ▁he / she ▁does ▁not ▁swear, ▁saying ▁that ▁a ▁person ▁who ▁is ▁not ▁a ▁victim ▁should\n",
            "2023-03-27 10:44:40 | INFO | fairseq.tasks.translation | example reference: ▁\" He ▁said ▁if ▁someone ▁hosts ▁investment ▁for ▁the ▁re f loating ▁expense ▁by ▁a ▁lie ▁that ▁a ▁person ▁discovered ▁a ▁treasure ▁ship, ▁that ▁ cro ok ▁should ▁be ▁put ▁to ▁sleep ▁by ▁people ▁who ▁are ▁not ▁victims, ▁but ▁he ▁didn ’ t ▁swear .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  65% 39/60 [00:46<00:30,  1.45s/it]\u001b[A2023-03-27 10:44:41 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" He ▁said, ▁“ Let 's ▁restore ▁our ▁resurrection ▁faith ▁to ▁celebrate ▁Easter ▁in ▁2019, ▁overcome ▁the ▁spiritual ▁slump ▁of ▁the ▁Korean ▁church, ▁the ▁economic ▁crisis ▁of ▁our ▁society, ▁and ▁security ▁instability. ”\"\n",
            "2023-03-27 10:44:41 | INFO | fairseq.tasks.translation | example reference: ▁\"\"\" Let 's ▁restore ▁our ▁faith ▁in ▁the ▁celebration ▁of ▁Easter ▁in ▁2016 ▁to ▁overcome ▁the ▁spiritual ▁downturn ▁of ▁the ▁Korean ▁church, ▁the ▁economic ▁crisis ▁( in ▁our ▁society), ▁and ▁security ▁concerns ,\"\" ▁he ▁said .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  67% 40/60 [00:48<00:29,  1.48s/it]\u001b[A2023-03-27 10:44:42 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" At ▁the ▁end ▁of ▁the ▁class ▁of ▁a ▁middle ▁school ▁in ▁Paris, ▁classical ▁music ▁sounds ▁instead ▁of ▁the ▁bell, ▁and ▁students ▁pour ▁out ▁of ▁the ▁door .\"\n",
            "2023-03-27 10:44:42 | INFO | fairseq.tasks.translation | example reference: ▁\" After ▁class ▁at ▁ Claude ▁De bus s y ▁Middle ▁School ▁in ▁Paris, ▁classical ▁music ▁rings ▁instead ▁of ▁bells ▁and ▁students ▁pour ▁out ▁the ▁doors .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  68% 41/60 [00:49<00:26,  1.41s/it]\u001b[A2023-03-27 10:44:44 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" A ▁total ▁of ▁14 2.4 ▁billion ▁won ▁will ▁be ▁spent ▁on ▁the ▁project, ▁and ▁as ▁early ▁as ▁the ▁end ▁of ▁June, ▁the ▁company ▁plans ▁to ▁place ▁an ▁order ▁to ▁investigate ▁and ▁design ▁Yangju ▁Techno ▁Valley ▁and ▁start ▁construction ▁in ▁2022 ▁with ▁the ▁aim ▁of ▁completing ▁it ▁in ▁20 24 .\"\n",
            "2023-03-27 10:44:44 | INFO | fairseq.tasks.translation | example reference: ▁A ▁total ▁of ▁14 24 ▁billion ▁won ▁will ▁be ▁invested. ▁The ▁company ▁plans ▁to ▁order ▁Yangju ▁Techno ▁Valley ▁research ▁and ▁design ▁services ▁as ▁early ▁as ▁June ▁and ▁start ▁construction ▁in ▁2022 ▁with ▁the ▁goal ▁of ▁completing ▁the ▁construction ▁in ▁2024.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  70% 42/60 [00:50<00:26,  1.45s/it]\u001b[A2023-03-27 10:44:45 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" In ▁2007, ▁the ▁Juk jeon ▁Station ▁was ▁completed ▁to ▁resolve ▁the ▁traffic ▁jam ▁in ▁the ▁Yongin ▁Juk jeon ▁district, ▁and ▁Giheung ▁Station ▁is ▁scheduled ▁to ▁open ▁at ▁the ▁end ▁of ▁2011, ▁two ▁years ▁earlier ▁than ▁the ▁original ▁one .\"\n",
            "2023-03-27 10:44:45 | INFO | fairseq.tasks.translation | example reference: ▁\" Ju k jeon ▁Station ▁was ▁completed ▁in ▁2007 ▁to ▁resolve ▁traffic ▁congestion ▁in ▁the ▁Yongin ▁Jukjeon ▁District. ▁Giheung ▁Station ▁is ▁scheduled ▁to ▁open ▁in ▁late ▁2011, ▁two ▁years ▁earlier ▁than ▁the ▁original ▁plan .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  72% 43/60 [00:51<00:22,  1.35s/it]\u001b[A2023-03-27 10:44:47 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" When ▁hiring ▁a ▁fixed - term ▁worker, ▁he / she ▁shall ▁prepare ▁and ▁conclude ▁a ▁written ▁labor ▁contract ▁in ▁attached ▁Form ▁2: ▁Provided, ▁That ▁in ▁consideration ▁of ▁the ▁characteristics ▁of ▁the ▁relevant ▁duties, ▁part ▁may ▁be ▁supplemented ▁and ▁operated .\"\n",
            "2023-03-27 10:44:47 | INFO | fairseq.tasks.translation | example reference: ▁\" When ▁employ ing ▁a ▁fixed - term ▁worker, ▁a ▁labor ▁contract ▁under ▁attached ▁Form ▁2 ▁shall ▁be ▁prepared ▁and ▁executed: ▁Provided, ▁That ▁in ▁consideration ▁of ▁the ▁characteristics ▁of ▁the ▁relevant ▁duties, ▁a ▁part ▁thereof ▁may ▁be ▁supplemented ▁and ▁operated .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  73% 44/60 [00:53<00:22,  1.40s/it]\u001b[A2023-03-27 10:44:48 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁Gwangyang ▁Hope ▁Library ▁will ▁display ▁the ▁completed ▁picture ▁books ▁with ▁the ▁original ▁picture ▁of ▁the ▁won ▁for ▁a ▁month ▁this ▁month, ▁and ▁register ▁them ▁as ▁library ▁books ▁for ▁a ▁month ▁to ▁keep ▁them ▁in ▁the ▁children 's ▁room ▁so ▁that ▁citizens ▁can ▁borrow ▁them .\"\n",
            "2023-03-27 10:44:48 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁Gwangyang ▁Hope ▁Library ▁will ▁display ▁the ▁completed ▁picture ▁books ▁with ▁won ▁paintings ▁for ▁one ▁month ▁this ▁month, ▁and ▁register ▁them ▁as ▁library ▁books ▁and ▁place ▁them ▁in ▁the ▁children 's ▁room ▁for ▁citizens ▁to ▁borrow ▁them .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  75% 45/60 [00:54<00:19,  1.31s/it]\u001b[A2023-03-27 10:44:49 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" I ▁think ▁it ▁would ▁be ▁good ▁if ▁parents ▁can ▁create ▁a ▁program ▁to ▁the ▁first ▁and ▁second ▁after ▁school ▁in ▁time ▁to ▁finish ▁their ▁work, ▁so ▁that ▁their ▁parents ▁can ▁leave ▁their ▁children ▁as ▁much ▁as ▁possible ▁until ▁late ▁at ▁night .\"\n",
            "2023-03-27 10:44:49 | INFO | fairseq.tasks.translation | example reference: ▁I ▁think ▁it ▁would ▁be ▁nice ▁if ▁they ▁could ▁make ▁programs ▁for ▁the ▁first ▁and ▁second ▁after ▁school ▁according ▁to ▁the ▁time ▁parents ▁finish ▁their ▁work ▁so ▁that ▁parents ▁can ▁work ▁freely ▁until ▁late ▁at ▁night.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  77% 46/60 [00:55<00:18,  1.32s/it]\u001b[A2023-03-27 10:44:51 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" After ▁this ▁process, ▁simulation s ▁were ▁operated ▁for ▁14 ▁buildings ▁in ▁the ▁section, ▁and ▁a ▁briefing ▁session ▁was ▁held ▁between ▁the ▁residents' ▁committee ▁and ▁the ▁promotion ▁company ▁to ▁discuss ▁difficulties ▁and ▁problems .\"\n",
            "2023-03-27 10:44:51 | INFO | fairseq.tasks.translation | example reference: ▁\" Through ▁this ▁process, ▁in ▁November, ▁it ▁operated ▁simulation s ▁for ▁14 ▁major ▁buildings ▁in ▁the ▁region, ▁had ▁a ▁report ▁meeting ▁with ▁resident ▁committee ▁and ▁the ▁companies, ▁and ▁discussed ▁difficulties ▁and ▁problems .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  78% 47/60 [00:57<00:17,  1.35s/it]\u001b[A2023-03-27 10:44:52 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Just ▁like ▁other ▁diseases, ▁mental ▁illness ▁is ▁good ▁when ▁you ▁find ▁it ▁early ▁and ▁receive ▁intensive ▁treatment ▁and ▁rehabilitation ▁services, ▁but ▁as ▁you ▁have ▁severe ▁falls, ▁you ▁can ▁hide ▁your ▁illness ▁and ▁worsen ▁the ▁problem .\"\n",
            "2023-03-27 10:44:52 | INFO | fairseq.tasks.translation | example reference: ▁\" As ▁with ▁other ▁diseases ▁mental ▁disorders ▁have ▁good ▁prognosis ▁when ▁found ▁early ▁and ▁treated ▁with ▁intensive ▁medical ▁care ▁and ▁rehabilitation ▁services, ▁but ▁the ▁more ▁severely ▁the ▁disease ▁is ▁being ▁judged, ▁the ▁more ▁the ▁patient ▁hide s ▁their ▁illness, ▁and ▁it ▁only ▁make ▁the ▁problem ▁worse .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  80% 48/60 [00:58<00:16,  1.40s/it]\u001b[A2023-03-27 10:44:54 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" It ▁is ▁said ▁that ▁viewers ▁were ▁able ▁to ▁sympathize ▁with ▁Do ▁Kyung - seok 's ▁first ▁love ▁and ▁concentrate ▁on ▁it ▁because ▁there ▁was ▁Cha ▁Eun - woo, ▁who ▁was ▁filling ▁up ▁the ▁emotional ▁lines ▁of ▁the ▁character ▁Do ▁Kyung - seok .\"\n",
            "2023-03-27 10:44:54 | INFO | fairseq.tasks.translation | example reference: ▁There ▁is ▁a ▁review ▁that ▁goes ▁the ▁reason ▁viewers ▁were ▁able ▁to ▁sympathize ▁with ▁and ▁be ▁immersed ▁in ▁Do ▁Kyung - seok ’ s ▁first ▁love ▁was ▁because ▁Cha ▁Eun - woo ▁who ▁is ▁acting ▁the ▁character ▁and ▁showing ▁emotional ▁lines ▁in ▁a ▁calm ▁and ▁order ly ▁way.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  82% 49/60 [01:00<00:16,  1.47s/it]\u001b[A2023-03-27 10:44:55 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" If ▁the ▁prosecution ▁fails ▁to ▁explain ▁the ▁link ▁in ▁detail ▁during ▁the ▁warrant ▁review, ▁the ▁court ▁could ▁reject ▁the ▁warrant, ▁saying, ▁\"\" We ▁need ▁to ▁guarantee ▁the ▁right ▁to ▁defend ▁because ▁there ▁is ▁a ▁dispute .\"\"\"\n",
            "2023-03-27 10:44:55 | INFO | fairseq.tasks.translation | example reference: ▁\" If ▁the ▁prosecution ▁fails ▁to ▁specifically ▁explain ▁this ▁Connect ing ▁Link ▁at ▁the ▁examination ▁of ▁the ▁warrant, ▁the ▁court ▁may ▁dismiss ▁the ▁warrant ▁by ▁saying, ▁\"\" We ▁need ▁to ▁guarantee ▁the ▁right ▁to ▁defense ▁because ▁there ▁is ▁a ▁struggle .\"\"\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  83% 50/60 [01:02<00:14,  1.49s/it]\u001b[A2023-03-27 10:44:57 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁Gyeonggi ▁Digital ▁Content ▁Agency ▁said ▁it ▁will ▁invite ▁major ▁overseas ▁buyers ▁to ▁the ▁2010 ▁Gyeonggi ▁ Technical ▁Game ▁Festival \"\"▁to ▁introduce ▁South ▁Korea 's ▁game, ▁which ▁boasts ▁a ▁world - class ▁level, ▁and ▁will ▁hold ▁game ▁export ▁consultations ▁involving ▁major ▁game ▁companies ▁in ▁Korea .\"\"\"\n",
            "2023-03-27 10:44:57 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁Gyeonggi ▁Digital ▁Content ▁Agency ▁said ▁it ▁will ▁invite ▁major ▁foreign ▁buyers ▁to ▁the ▁2010 ▁Gyeonggi ▁Functional ▁Game ▁Festival \"\"▁to ▁introduce ▁South ▁Korea 's ▁games ▁boasting ▁a ▁world - class ▁level, ▁and ▁hold ▁game ▁export ▁counseling ▁sessions ▁involving ▁major ▁Korean ▁game ▁companies .\"\"\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  85% 51/60 [01:03<00:13,  1.52s/it]\u001b[A2023-03-27 10:44:58 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Market ▁research ▁firm ▁St. A. A., ▁a ▁market ▁research ▁firm, ▁analyzed ▁that ▁Samsung ▁Electronics ▁shipped ▁2, 94 6 ▁million ▁smartphones ▁last ▁year, ▁marking ▁the ▁first ▁shipment ▁price ▁of ▁less ▁than ▁300 ▁million ▁since ▁2013.\"\n",
            "2023-03-27 10:44:58 | INFO | fairseq.tasks.translation | example reference: ▁\" S t rate gy ▁Analytics (S A), ▁a ▁market ▁research ▁firm, ▁analyzed ▁that ▁Samsung ▁Electronics ▁shipped ▁29 4.6 ▁million ▁smartphones ▁last ▁year, ▁and ▁they ▁might ▁have ▁recorded ▁the ▁shipments ▁less ▁than ▁300 ▁million ▁for ▁the ▁first ▁time ▁since ▁2013.\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  87% 52/60 [01:04<00:11,  1.46s/it]\u001b[A2023-03-27 10:45:00 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" Four ▁out ▁of ▁13 ▁people ▁belonging ▁to ▁the ▁youth ▁football ▁team ▁\"\" Mu m ▁fast ,\"\" ▁which ▁had ▁been ▁trapped ▁for ▁16 ▁days ▁in ▁Lu ang ▁cave, ▁Chiang ▁Rai, ▁Thailand, ▁were ▁dramatically ▁rescued ▁on ▁the ▁8 th ▁( local ▁time ).\"\n",
            "2023-03-27 10:45:00 | INFO | fairseq.tasks.translation | example reference: ▁\" 4 ▁people ▁out ▁of ▁the ▁13 ▁people ▁from ▁the ▁youth ▁soccer ▁team ▁Moo ▁Pa ▁( Wild ▁Bo ar), ▁who ▁were ▁trapped ▁in ▁the ▁T ham ▁Lu ang ▁cave ▁in ▁Chiang ▁Rai ▁Province ▁for ▁16 ▁days, ▁have ▁been ▁dramatically ▁rescued ▁on ▁8 th ▁( hereinafter ▁local ▁time ).\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  88% 53/60 [01:06<00:10,  1.47s/it]\u001b[A2023-03-27 10:45:01 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" According ▁to ▁the ▁2015 ▁Food ▁and ▁Drug ▁Statistics ▁Report, ▁the ▁medical ▁device ▁industry ▁is ▁a ▁high ▁value - added ▁industry ▁with ▁an ▁average ▁annual ▁growth ▁of ▁2.6 6%, ▁and ▁continuous ▁high ▁growth ▁is ▁expected ▁as ▁global ▁economic ▁growth, ▁aging ▁and ▁health ▁interest ▁increases .\"\n",
            "2023-03-27 10:45:01 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁medical ▁device ▁industry ▁continues ▁to ▁grow ▁at ▁an ▁average ▁annual ▁rate ▁of ▁6.2 6% ▁according ▁to ▁the ▁2015 ▁Food ▁and ▁Drug ▁Statistics ▁Annual ▁Report ▁and ▁is ▁a ▁high ▁value - added ▁industry ▁that ▁is ▁expected ▁to ▁sustain ▁high ▁growth ▁in ▁line ▁with ▁global ▁economic ▁growth, ▁aging ▁population ▁and ▁growing ▁interest ▁in ▁health .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  90% 54/60 [01:07<00:08,  1.44s/it]\u001b[A2023-03-27 10:45:02 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" During ▁the ▁Chuseok ▁holiday, ▁Siheung, ▁Bucheon ▁and ▁Osan, ▁which ▁can ▁share ▁vehicles ▁among ▁cities ▁and ▁counties ▁in ▁the ▁province, ▁participated ▁as ▁pilot ▁projects ▁to ▁prepare ▁a ▁total ▁of ▁123 ▁vehicles, ▁including ▁105 ▁existing ▁vehicles ▁owned ▁by ▁Gyeonggi ▁Province, ▁10 ▁cars ▁owned ▁by ▁Siheung, ▁3 ▁cars ▁in ▁Bucheon ▁and ▁5 ▁cars ▁in ▁Osan .\"\n",
            "2023-03-27 10:45:02 | INFO | fairseq.tasks.translation | example reference: ▁\" In ▁this ▁Chuseok ▁holiday, ▁the ▁Siheung, ▁Bucheon, ▁and ▁Osan ▁city, ▁which ▁can ▁share ▁vehicles, ▁participated ▁in ▁pilot ▁project ▁and ▁prepared ▁a ▁total ▁of ▁123 ▁including ▁105 ▁vehicles ▁owned ▁by ▁Gyeonggi - do, ▁10 ▁from ▁Siheung, ▁3 ▁from ▁Bucheon, ▁and ▁5 ▁from ▁Osan .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  92% 55/60 [01:09<00:07,  1.42s/it]\u001b[A2023-03-27 10:45:04 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" There ▁are ▁a ▁wide ▁range ▁of ▁menus ▁such ▁as ▁dried ▁water ▁& ▁spicy ▁spicy ▁noodles, ▁hot ▁noodles, ▁etc. ▁that ▁can ▁be ▁enjoyed ▁as ▁a ▁star, ▁and ▁there ▁are ▁also ▁a ▁variety ▁of ▁menus ▁such ▁as ▁handmade ▁pork ▁bars ▁that ▁children ▁can ▁enjoy .\"\n",
            "2023-03-27 10:45:04 | INFO | fairseq.tasks.translation | example reference: ▁\" There ▁are ▁buckwheat ▁water ▁& ▁bibim ▁ mak kuk soo, ▁whole ▁buckwheat ▁noodles, ▁buckwheat ▁ muk sa bal ▁that ▁can ▁be ▁enjoyed ▁as ▁a ▁delicacy, ▁and ▁also ▁a ▁variety ▁of ▁menu ▁items ▁including ▁handmade ▁big ▁pork ▁cutlet ▁that ▁children ▁can ▁enjoy .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  93% 56/60 [01:10<00:05,  1.43s/it]\u001b[A2023-03-27 10:45:05 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁purpose ▁of ▁this ▁Ordinance ▁is ▁to ▁contribute ▁to ▁the ▁respect ▁of ▁life ▁of ▁animals, ▁the ▁promotion ▁of ▁welfare ▁of ▁Do ▁residents, ▁and ▁the ▁cultivation ▁of ▁emotions ▁of ▁Do ▁residents, ▁such ▁as ▁protecting ▁and ▁managing ▁animals ▁appropriately ▁and ▁preventing ▁the ▁occurrence ▁of ▁lost ▁or ▁abandoned ▁animals ▁pursua nt ▁to ▁the ▁Animal ▁Protection ▁Act .\"\n",
            "2023-03-27 10:45:05 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁purpose ▁of ▁this ▁Ordinance ▁is ▁to ▁properly ▁protect ▁and ▁manage ▁animals ▁and ▁prevent ▁generation ▁of ▁abandoned ▁or ▁lost ▁animals, ▁thereby ▁contributing ▁to ▁enhancing ▁the ▁respect ▁for ▁the ▁life ▁of ▁animals ▁and ▁their ▁welfare ▁as ▁well ▁as ▁developing ▁e th os ▁of ▁Do ▁residents ▁in ▁accordance ▁with ▁the ▁Animal ▁Protection ▁Act .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  95% 57/60 [01:12<00:04,  1.41s/it]\u001b[A2023-03-27 10:45:07 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" In ▁the ▁meantime, ▁the ▁Japanese ▁media ▁showed ▁interest ▁in ▁the ▁Declaration ▁of ▁Independence ▁of ▁the ▁March ▁1 st ▁Movement, ▁and ▁according ▁to ▁Christian ▁teaching, ▁they ▁decided ▁to ▁donate ▁the ▁Declaration ▁of ▁Independence ▁to ▁the ▁Independence ▁Memorial ▁Hall ▁with ▁the ▁idea ▁that ▁\"\" What 's ▁the ▁case ▁of ▁Ka s sa sa \"\"▁in ▁accordance ▁with ▁Christian ▁teaching .\"\n",
            "2023-03-27 10:45:07 | INFO | fairseq.tasks.translation | example reference: ▁\" In ▁the ▁meantime, ▁on ▁the ▁occasion ▁of ▁the ▁100 th ▁anniversary ▁of ▁the ▁March ▁1 ▁Independence ▁Movement, ▁Japanese ▁media ▁showed ▁interest ▁in ▁Masa o ▁Sa to 's ▁Declaration ▁of ▁Independence ▁and ▁decided ▁to ▁donate ▁the ▁declaration ▁to ▁the ▁Independence ▁Hall ▁in ▁the ▁belief ▁that ▁\"\" the ▁one ▁that ▁belongs ▁to ▁Caesar ▁to ▁Caesar \"\"▁according ▁to ▁Christian ▁teaching s .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  97% 58/60 [01:13<00:02,  1.48s/it]\u001b[A2023-03-27 10:45:09 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" The ▁term ▁\"\" person ▁with ▁severe ▁disabilities \"\"▁means ▁a ▁person ▁who ▁needs ▁help ▁from ▁a ▁person ▁who ▁is ▁greatly ▁restricted ▁by ▁daily ▁life ▁or ▁social ▁life ▁due ▁to ▁the ▁disability ▁of ▁language, ▁body ▁expression, ▁self - regulation, ▁social ▁adaptation ▁function, ▁and ▁ability ▁under ▁Article ▁2 ▁of ▁the ▁Act ▁and ▁Article ▁2 ▁of ▁the ▁Enforcement ▁Decree ▁of ▁the ▁same ▁Act .\"\n",
            "2023-03-27 10:45:09 | INFO | fairseq.tasks.translation | example reference: ▁\" The ▁term ▁\"\" disabled ▁person ▁with ▁autism \"\"▁means ▁a ▁person ▁who ▁needs ▁help ▁from ▁the ▁others ▁due ▁to ▁considerable ▁restriction ▁in ▁daily ▁life ▁or ▁social ▁life ▁because ▁of ▁the ▁disabilities ▁of ▁abilities ▁and ▁functions ▁in ▁speech, ▁physical ▁expression, ▁self - regulation, ▁and ▁adaptation ▁to ▁society ▁pursua nt ▁to ▁childhood ▁autism ▁and ▁non - typical ▁autism ▁prescribed ▁in ▁Article ▁2 ▁of ▁the ▁Act ▁and ▁Article ▁2 ▁of ▁the ▁Enforcement ▁Decree ▁of ▁the ▁same ▁Act .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  98% 59/60 [01:15<00:01,  1.64s/it]\u001b[A2023-03-27 10:45:10 | INFO | fairseq.tasks.translation | example hypothesis: ▁\" According ▁to ▁the ▁2016 ▁data ▁released ▁by ▁the ▁National ▁Human ▁Rights ▁Commission ▁of ▁Korea, ▁women ▁(1, 2 79 ▁people) ▁had ▁about ▁1.8 ▁times ▁more ▁than ▁men ▁(6, 2 64 ▁people) ▁as ▁of ▁2002, ▁but ▁in ▁2015, ▁13 ▁years ▁later, ▁the ▁gap ▁widened ▁to ▁41, 17 4 ▁men ▁and ▁17, 4 46 ▁men .\"\n",
            "2023-03-27 10:45:10 | INFO | fairseq.tasks.translation | example reference: ▁\" According ▁to ▁data ▁released ▁by ▁the ▁National ▁Human ▁Rights ▁Commission ▁of ▁Korea ▁of ▁2016, ▁there ▁were ▁about ▁1.8 ▁times ▁as ▁many ▁short - term ▁workers ▁as ▁women ▁in ▁2002 ▁(12, 27 9) ▁than ▁men ▁( 66, 26 4), ▁but ▁after ▁13 ▁years ▁later ▁in ▁2015, ▁the ▁gap ▁was ▁up ▁to ▁2.4 ▁times, ▁with ▁1,3 07 ▁women ▁and ▁17, 14 6 ▁men .\"\n",
            "\n",
            "epoch 005 | valid on 'valid' subset: 100% 60/60 [01:17<00:00,  1.55s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-03-27 10:45:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.041 | nll_loss 2.321 | ppl 5 | bleu 34.58 | wps 1933.3 | wpb 2475.7 | bsz 83.3 | num_updates 59120 | best_bleu 34.58\n",
            "2023-03-27 10:45:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 59120 updates\n",
            "2023-03-27 10:45:10 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_best.pt\n",
            "2023-03-27 10:45:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_best.pt\n",
            "2023-03-27 10:45:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_best.pt (epoch 5 @ 59120 updates, score 34.58) (writing took 8.759344830999908 seconds)\n",
            "2023-03-27 10:45:19 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-03-27 10:45:19 | INFO | train | epoch 005 | loss 4.22 | nll_loss 2.633 | ppl 6.2 | wps 8367.5 | ups 2.22 | wpb 3765.5 | bsz 126.3 | num_updates 59120 | lr 0.000130057 | gnorm 1.221 | train_wall 5173 | gb_free 11.1 | wall 5327\n",
            "2023-03-27 10:45:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-03-27 10:45:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 11824\n",
            "epoch 006:   0% 0/11824 [00:00<?, ?it/s]2023-03-27 10:45:19 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-03-27 10:45:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 574, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 205, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.9/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 331, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.9/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 843, in train_step\n",
            "    loss, sample_size_i, logging_output = self.task.train_step(\n",
            "  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 536, in train_step\n",
            "    optimizer.backward(loss)\n",
            "  File \"/content/fairseq/fairseq/optim/fairseq_optimizer.py\", line 96, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\", line 488, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-generate /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/ \\\n",
        "--path /content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_best.pt \\\n",
        "--source-lang ko --target-lang en \\\n",
        "--max-sentences 100 \\\n",
        "--beam 5 \\\n",
        "--task translation \\\n",
        "--gen-subset test \\\n",
        "--skip-invalid-size-inputs-valid-test \\\n",
        "--batch-size 64 \\\n",
        "--remove-bpe sentencepiece \\\n",
        "--results-path prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT0yI160RYpJ",
        "outputId": "bd4768aa-65fb-4bfc-b3dd-e6b148531338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-27 11:04:19 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': 'prediction'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 64, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': '/content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/', 'source_lang': 'ko', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-03-27 11:04:19 | INFO | fairseq.tasks.translation | [ko] dictionary: 31176 types\n",
            "2023-03-27 11:04:19 | INFO | fairseq.tasks.translation | [en] dictionary: 31816 types\n",
            "2023-03-27 11:04:19 | INFO | fairseq_cli.generate | loading model(s) from /content/drive/MyDrive/AllforOne/Lecture/Fairseq/checkpoints/checkpoint_best.pt\n",
            "2023-03-27 11:04:23 | INFO | fairseq.data.data_utils | loaded 3,000 examples from: /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/test.ko-en.ko\n",
            "2023-03-27 11:04:23 | INFO | fairseq.data.data_utils | loaded 3,000 examples from: /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/test.ko-en.en\n",
            "2023-03-27 11:04:23 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/AllforOne/Lecture/Fairseq/preprocess/ test ko-en 3000 examples\n",
            "2023-03-27 11:04:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2023-03-27 11:04:25 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2023-03-27 11:04:25 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2023-03-27 11:04:25 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2023-03-27 11:05:20 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-03-27 11:05:20 | INFO | fairseq_cli.generate | Translated 3,000 sentences (89,542 tokens) in 37.1s (80.75 sentences/s, 2410.32 tokens/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep '^H' prediction/generate-test.txt | cut -f3- > prediction/gen.out.sys # 예측된 문장 (H)\n",
        "!grep '^T' prediction/generate-test.txt | cut -f2- > prediction/gen.out.ref # 타겟(정답) 문장 (T)"
      ],
      "metadata": {
        "id": "KmaRNGYvwTqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-score \\\n",
        "--sys prediction/gen.out.sys \\\n",
        "--ref prediction/gen.out.ref"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKZr5AmfR8RS",
        "outputId": "2c148320-796e-4de3-e1ea-9edf5ccc579b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(sys='prediction/gen.out.sys', ref='prediction/gen.out.ref', order=4, ignore_case=False, sacrebleu=False, sentence_bleu=False)\n",
            "BLEU4 = 5.42, 25.4/7.4/3.0/1.5 (BP=1.000, ratio=1.051, syslen=74620, reflen=70990)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate test result (BLEU)\n",
        "!tail -n 1 /content/drive/MyDrive/AllforOne/Lecture/Fairseq/prediction/generate-test.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9jjKAThwx9S",
        "outputId": "76326cde-3e1c-4c31-bb0c-aa633150e55f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "Generate test with beam=5: BLEU4 = 28.36, 56.6/34.3/22.6/15.6 (BP=0.985, ratio=0.985, syslen=69947, reflen=70990)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhdTLN-VvuXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}